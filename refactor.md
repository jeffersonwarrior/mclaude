# MClaude Refactor to TensorZero Proxy Architecture\n\n## Current Architecture Review\n\n**Alignment with described architecture:**\n- ✅ **Providers**: synthetic (`https://api.synthetic.new/anthropic`) and minimax (`https://api.minimax.io/anthropic`) - exact match.\n- ✅ **Proxy**: Local proxy on `127.0.0.1:9313` provides Anthropic endpoint.\n- ⚠️ **Current Proxy**: Uses LiteLLM (`src/router/litellm-proxy.ts`), spawns `litellm` CLI with hardcoded Synthetic DeepSeek model.\n- ⚠️ **TensorZero Proxy**: Prototype exists (`src/router/tensorzero-proxy.ts`) but not active. Configures multiple models dynamically from config keys.\n- ✅ **Routing**: Config supports `minimax:*` / `synthetic:*` patterns (LiteLLM), TensorZero uses explicit `models[]` list with `name` → backend mapping (e.g. `synthetic_deepseek` → `deepseek-ai/DeepSeek-V3.2`).\n- ✅ **Config**: `recommendedModels` (thinking, subagent, fast, normal) maps roles to models (currently `hf:*` prefixed).\n- ✅ **Env Standardization**: Launcher sets `ANTHROPIC_BASE_URL` to proxy, `ANTHROPIC_API_KEY=sk-${key}`.\n- ❌ **LiteLLM Issues**: Hardcoded single model, SQLite hacks, version pinning, pip deps.\n\n**Gaps**:\n- Switch to TensorZero.\n- Full Synthetic models list (22+ `hf:*` models listed).\n- Minimax: Only `MiniMax-m1`, `MiniMax-M2`.\n- Model name mapping: Prefix routing (`minimax:M2`) → backend.\n- Configurable role models → proxy `name`s.\n\n## Refactor TODOs (Priority Order)\n\n### 1. **Switch to TensorZero Proxy (Critical)**\n   - [ ] Update `src/router/manager.ts`: Import/use `TensorZeroProxy` instead of `LiteLLMProxy`.\n   - [ ] Update config schema `src/config/types.ts`: Rename `liteLLM` → `tensorzero` (keep `enabled/port/host/timeout`).\n   - [ ] Test: `npm test` + manual `mclaude doctor`.\n\n### 2. **Expand TensorZero Models List**\n   - [ ] `src/router/tensorzero-proxy.ts#createTensorZeroConfig()`: Generate full `models[]` from user list.\n     ```ts\n     Synthetic models: [\n       'hf:MiniMaxAI/MiniMax-M2',\n       'hf:moonshotai/Kimi-K2-Thinking',\n       // ... all 22\n     ]\n     Minimax: ['MiniMax-m1', 'MiniMax-M2']\n     ```\n   - [ ] Prefix mapping: `name: 'synthetic_${model}'` or `minimax:${model_name}`.\n   - [ ] Backend: `provider: 'openai'`, `model_name`, `api_base`, `api_key` per provider.\n\n### 3. **Role Model Mappings (Thinking/SubAgent/Fast/Normal)**\n   - [ ] Update `recommendedModels` defaults to proxy names (e.g. `thinking.primary: 'minimax:M2'`).\n   - [ ] UI/CLI: Model selection → proxy `name` (not backend `hf:*`).\n   - [ ] Launcher: Request proxy model names directly.\n\n### 4. **Standardize Anthropic Env Vars**\n   - [ ] Fixed: `ANTHROPIC_BASE_URL=http://127.0.0.1:9313`, `ANTHROPIC_API_KEY=sk-master` (proxy-internal).\n   - [ ] Remove provider-specific base/key overrides in launcher.\n   - [ ] Proxy handles all routing/mapping.\n\n### 5. **Remove LiteLLM Dependencies**\n   - [ ] `package.json`: Remove `litellm` npm dep, `install-litellm` script, `postinstall`.\n   - [ ] Add `install-tensorzero: pip install tensorzero`.\n   - [ ] Delete `src/router/litellm-proxy.ts`, Prisma/SQLite hacks.\n   - [ ] Update CRUSH.md, LITELLM_*.md → TensorZero docs.\n\n### 6. **Config & Schema Updates**\n   - [ ] `src/config/types.ts`: Add `tensorzero.models` array for custom overrides.\n   - [ ] Migrate existing configs: `liteLLM.enabled=true` → `tensorzero.enabled=true`.\n   - [ ] Add `mclaude setup --tensorzero` command.\n\n### 7. **Testing & Validation**\n   - [ ] Unit: Mock TensorZero spawn, test config generation.\n   - [ ] Integration: `mclaude models` lists proxy models.\n   - [ ] E2E: `mclaude --model minimax:M2` → routes correctly.\n   - [ ] Doctor: Verify proxy health, models list.\n\n### 8. **Docs & Cleanup**\n   - [ ] README.md: New architecture diagram.\n   - [ ] CHANGELOG.md: v1.7.0 TensorZero migration.\n   - [ ] Update GitHub workflows if needed.\n\n## Migration Plan\n1. Implement 1-2 → Test proxy switch.\n2. 3-4 → Role mappings & env.\n3. 5-6 → Cleanup.\n4. 7-8 → Polish.\n\n**Risks**: TensorZero stability (newer tool), pip deps, model compatibility.\n**Fallback**: Keep LiteLLM optional via config flag.